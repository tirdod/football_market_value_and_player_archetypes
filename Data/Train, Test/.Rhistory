y = expression(eta[1]),
fill = "BIC"
) +
theme_minimal()
# Chunk 1
#––– PACKAGES –––
library(readr)
library(glmnet)
library(pROC)
library(caret)
library(ParBayesianOptimization)
library(dplyr)
# Chunk 2
#––– 0) LOAD + PREP DATA –––
#gene_expression_liver <- read_csv("/Users/inigo/Downloads/gene_expression_only_liver_final.csv") %>%
#  select(-Is_stage_34, -id)
#––– 0) LOAD + PREP DATA –––
gene_expression_liver <- read_csv(
"~/Desktop/BSE/Term 3/Thesis/liver cancer/gene_expression_only_liver_final.csv",
show_col_types = FALSE
) %>%
select(-Is_stage_34, -id)
y <- gene_expression_liver$OS
X <- scale(gene_expression_liver %>% select(-OS)) %>% as.matrix()
p <- ncol(X)
predictor_names <- colnames(X)
#––– 0a) READ PRIOR ADAPTIVE‐RIDGE COEFFICIENTS –––
coef_ridge <- read_csv(
"~/Downloads/Breast_OS_coefficients_adaptive_Ridge.csv",
show_col_types = FALSE
) %>%
mutate(abs_value = abs(value))
# Chunk 3
# Step 2: Compute threshold at 25th percentile
cutoff <- quantile(coef_ridge$abs_value, 0.25)
# Step 3: Create binary z_j indicator: 1 if in top 75%, 0 otherwise
coef_ridge <- coef_ridge %>%
mutate(z_j = as.integer(abs_value > cutoff))
# Step 4: Initialize z_j vector for all predictors
z_j <- integer(p)
names(z_j) <- predictor_names
# Step 5: Assign z_j values for predictors found in coef_ridge
matched <- match(coef_ridge$Variable, predictor_names)
z_j[matched[!is.na(matched)]] <- coef_ridge$z_j[!is.na(matched)]
# Chunk 4
compute_bic <- function(eta0, eta1, X, y, z_j) {
n <- nrow(X)
# 1) compute lambda_j using binary z_j
lambda_j <- exp(eta0 + eta1 * z_j)
# 2) scale design matrix by lambda_j
W <- sweep(X, 2, lambda_j, FUN = "/")
# 3) fit ridge regression with fixed lambda = 1
fit <- glmnet(
x           = W,
y           = y,
family      = "binomial",
alpha       = 0,
lambda      = 1,
intercept   = TRUE,
standardize = FALSE
)
coefs      <- as.numeric(coef(fit, s = 1))
intercept  <- coefs[1]
beta_tilde <- coefs[-1]
beta_j     <- beta_tilde / lambda_j
# 4) compute log-likelihood
linpred <- intercept + X %*% beta_j
p_hat   <- pmin(pmax(1 / (1 + exp(-linpred)), 1e-8), 1 - 1e-8)
ll      <- sum(y * log(p_hat) + (1 - y) * log(1 - p_hat))
# 5) compute BIC
df_beta <- ncol(X)
df_eta  <- 1 + as.integer(eta1 != 0)
df_tot  <- df_beta + df_eta
bic_val <- -2 * ll + df_tot * log(n)
if (!is.finite(bic_val)) bic_val <- 1e10
return(bic_val)
}
# Chunk 5
#––– GRID SEARCH over eta0, eta1 –––
eta0_grid <- seq(-5, -2, by = 0.25)   # baseline penalty
eta1_grid <- seq(-2,  2, by = 0.25)   # penalty adjustment for z_j
# build all combinations
grid <- expand.grid(eta0 = eta0_grid, eta1 = eta1_grid)
# evaluate BIC at each point using z_j
grid$bic <- mapply(
compute_bic,
eta0     = grid$eta0,
eta1     = grid$eta1,
MoreArgs = list(X = X, y = y, z_j = z_j)
)
# find the best hyperparameter combination
best_idx <- which.min(grid$bic)
best     <- grid[best_idx, ]
cat(sprintf(
"→ Best BIC: η₀ = %.3f, η₁ = %.3f, BIC = %.1f\n",
best$eta0, best$eta1, best$bic
))
# Chunk 6
#––– 0) LOAD + PREP DATA –––
#gene_expression_liver <- read_csv("/Users/inigo/Downloads/gene_expression_only_liver_final.csv") %>%
#  select(-Is_stage_34, -id)
#––– 0) LOAD + PREP DATA –––
gene_expression_liver <- read_csv(
"~/Desktop/BSE/Term 3/Thesis/liver cancer/gene_expression_only_liver_final.csv",
show_col_types = FALSE
) %>%
select(-OS, -id)
y <- gene_expression_liver$Is_stage_34
X <- scale(gene_expression_liver %>% select(-Is_stage_34)) %>% as.matrix()
p <- ncol(X)
predictor_names <- colnames(X)
#––– 0a) READ PRIOR ADAPTIVE‐RIDGE COEFFICIENTS –––
coef_ridge <- read_csv(
"~/Downloads/Breast_isstage_coefficients_adaptive_Ridge.csv",
show_col_types = FALSE
) %>%
mutate(abs_value = abs(value))
# Chunk 7
# Step 2: Compute threshold at 25th percentile
cutoff <- quantile(coef_ridge$abs_value, 0.25)
# Step 3: Create binary z_j indicator: 1 if in top 75%, 0 otherwise
coef_ridge <- coef_ridge %>%
mutate(z_j = as.integer(abs_value > cutoff))
# Step 4: Initialize z_j vector for all predictors
z_j <- integer(p)
names(z_j) <- predictor_names
# Step 5: Assign z_j values for predictors found in coef_ridge
matched <- match(coef_ridge$Variable, predictor_names)
z_j[matched[!is.na(matched)]] <- coef_ridge$z_j[!is.na(matched)]
# Chunk 8
compute_bic <- function(eta0, eta1, X, y, z_j) {
n <- nrow(X)
# 1) compute lambda_j using binary z_j
lambda_j <- exp(eta0 + eta1 * z_j)
# 2) scale design matrix by lambda_j
W <- sweep(X, 2, lambda_j, FUN = "/")
# 3) fit ridge regression with fixed lambda = 1
fit <- glmnet(
x           = W,
y           = y,
family      = "binomial",
alpha       = 0,
lambda      = 1,
intercept   = TRUE,
standardize = FALSE
)
coefs      <- as.numeric(coef(fit, s = 1))
intercept  <- coefs[1]
beta_tilde <- coefs[-1]
beta_j     <- beta_tilde / lambda_j
# 4) compute log-likelihood
linpred <- intercept + X %*% beta_j
p_hat   <- pmin(pmax(1 / (1 + exp(-linpred)), 1e-8), 1 - 1e-8)
ll      <- sum(y * log(p_hat) + (1 - y) * log(1 - p_hat))
# 5) compute BIC
df_beta <- ncol(X)
df_eta  <- 1 + as.integer(eta1 != 0)
df_tot  <- df_beta + df_eta
bic_val <- -2 * ll + df_tot * log(n)
if (!is.finite(bic_val)) bic_val <- 1e10
return(bic_val)
}
# Chunk 9
#––– GRID SEARCH over eta0, eta1 –––
eta0_grid <- seq(-5, -2, by = 0.25)   # baseline penalty
eta1_grid <- seq(-2,  2, by = 0.25)   # penalty adjustment for z_j
# build all combinations
grid <- expand.grid(eta0 = eta0_grid, eta1 = eta1_grid)
# evaluate BIC at each point using z_j
grid$bic <- mapply(
compute_bic,
eta0     = grid$eta0,
eta1     = grid$eta1,
MoreArgs = list(X = X, y = y, z_j = z_j)
)
# find the best hyperparameter combination
best_idx <- which.min(grid$bic)
best     <- grid[best_idx, ]
cat(sprintf(
"→ Best BIC: η₀ = %.3f, η₁ = %.3f, BIC = %.1f\n",
best$eta0, best$eta1, best$bic
))
# Chunk 1
#––– PACKAGES –––
library(readr)
library(glmnet)
library(pROC)
library(caret)
library(ParBayesianOptimization)
library(dplyr)
# Chunk 2
#––– 0) LOAD + PREP DATA –––
#gene_expression_liver <- read_csv("/Users/inigo/Downloads/gene_expression_only_liver_final.csv") %>%
#  select(-Is_stage_34, -id)
#––– 0) LOAD + PREP DATA –––
gene_expression_liver <- read_csv(
"~/Desktop/BSE/Term 3/Thesis/liver cancer/gene_expression_only_liver_final.csv",
show_col_types = FALSE
) %>%
select(-Is_stage_34, -id)
y <- gene_expression_liver$OS
X <- scale(gene_expression_liver %>% select(-OS)) %>% as.matrix()
p <- ncol(X)
predictor_names <- colnames(X)
#––– 0a) READ PRIOR ADAPTIVE‐RIDGE COEFFICIENTS –––
coef_ridge <- read_csv(
"~/Downloads/Breast_OS_coefficients_adaptive_Ridge.csv",
show_col_types = FALSE
) %>%
mutate(abs_value = abs(value))
# Chunk 3
# Step 2: Compute threshold at 25th percentile
cutoff <- quantile(coef_ridge$abs_value, 0.25)
# Step 3: Create binary z_j indicator: 1 if in top 75%, 0 otherwise
coef_ridge <- coef_ridge %>%
mutate(z_j = as.integer(abs_value > cutoff))
# Step 4: Initialize z_j vector for all predictors
z_j <- integer(p)
names(z_j) <- predictor_names
# Step 5: Assign z_j values for predictors found in coef_ridge
matched <- match(coef_ridge$Variable, predictor_names)
z_j[matched[!is.na(matched)]] <- coef_ridge$z_j[!is.na(matched)]
# Chunk 4
compute_bic <- function(eta0, eta1, X, y, z_j) {
n <- nrow(X)
# 1) compute lambda_j using binary z_j
lambda_j <- exp(eta0 + eta1 * z_j)
# 2) scale design matrix by lambda_j
W <- sweep(X, 2, lambda_j, FUN = "/")
# 3) fit ridge regression with fixed lambda = 1
fit <- glmnet(
x           = W,
y           = y,
family      = "binomial",
alpha       = 0,
lambda      = 1,
intercept   = TRUE,
standardize = FALSE
)
coefs      <- as.numeric(coef(fit, s = 1))
intercept  <- coefs[1]
beta_tilde <- coefs[-1]
beta_j     <- beta_tilde / lambda_j
# 4) compute log-likelihood
linpred <- intercept + X %*% beta_j
p_hat   <- pmin(pmax(1 / (1 + exp(-linpred)), 1e-8), 1 - 1e-8)
ll      <- sum(y * log(p_hat) + (1 - y) * log(1 - p_hat))
# 5) compute BIC
df_beta <- ncol(X)
df_eta  <- 1 + as.integer(eta1 != 0)
df_tot  <- df_beta + df_eta
bic_val <- -2 * ll + df_tot * log(n)
if (!is.finite(bic_val)) bic_val <- 1e10
return(bic_val)
}
# Chunk 5
#––– GRID SEARCH over eta0, eta1 –––
eta0_grid <- seq(-10, -2, by = 0.25)   # baseline penalty
eta1_grid <- seq(-2,  2, by = 0.25)   # penalty adjustment for z_j
# build all combinations
grid <- expand.grid(eta0 = eta0_grid, eta1 = eta1_grid)
# evaluate BIC at each point using z_j
grid$bic <- mapply(
compute_bic,
eta0     = grid$eta0,
eta1     = grid$eta1,
MoreArgs = list(X = X, y = y, z_j = z_j)
)
# find the best hyperparameter combination
best_idx <- which.min(grid$bic)
best     <- grid[best_idx, ]
cat(sprintf(
"→ Best BIC: η₀ = %.3f, η₁ = %.3f, BIC = %.1f\n",
best$eta0, best$eta1, best$bic
))
# Chunk 6
#––– 0) LOAD + PREP DATA –––
#gene_expression_liver <- read_csv("/Users/inigo/Downloads/gene_expression_only_liver_final.csv") %>%
#  select(-Is_stage_34, -id)
#––– 0) LOAD + PREP DATA –––
gene_expression_liver <- read_csv(
"~/Desktop/BSE/Term 3/Thesis/liver cancer/gene_expression_only_liver_final.csv",
show_col_types = FALSE
) %>%
select(-OS, -id)
y <- gene_expression_liver$Is_stage_34
X <- scale(gene_expression_liver %>% select(-Is_stage_34)) %>% as.matrix()
p <- ncol(X)
predictor_names <- colnames(X)
#––– 0a) READ PRIOR ADAPTIVE‐RIDGE COEFFICIENTS –––
coef_ridge <- read_csv(
"~/Downloads/Breast_isstage_coefficients_adaptive_Ridge.csv",
show_col_types = FALSE
) %>%
mutate(abs_value = abs(value))
# Chunk 7
# Step 2: Compute threshold at 25th percentile
cutoff <- quantile(coef_ridge$abs_value, 0.25)
# Step 3: Create binary z_j indicator: 1 if in top 75%, 0 otherwise
coef_ridge <- coef_ridge %>%
mutate(z_j = as.integer(abs_value > cutoff))
# Step 4: Initialize z_j vector for all predictors
z_j <- integer(p)
names(z_j) <- predictor_names
# Step 5: Assign z_j values for predictors found in coef_ridge
matched <- match(coef_ridge$Variable, predictor_names)
z_j[matched[!is.na(matched)]] <- coef_ridge$z_j[!is.na(matched)]
# Chunk 8
compute_bic <- function(eta0, eta1, X, y, z_j) {
n <- nrow(X)
# 1) compute lambda_j using binary z_j
lambda_j <- exp(eta0 + eta1 * z_j)
# 2) scale design matrix by lambda_j
W <- sweep(X, 2, lambda_j, FUN = "/")
# 3) fit ridge regression with fixed lambda = 1
fit <- glmnet(
x           = W,
y           = y,
family      = "binomial",
alpha       = 0,
lambda      = 1,
intercept   = TRUE,
standardize = FALSE
)
coefs      <- as.numeric(coef(fit, s = 1))
intercept  <- coefs[1]
beta_tilde <- coefs[-1]
beta_j     <- beta_tilde / lambda_j
# 4) compute log-likelihood
linpred <- intercept + X %*% beta_j
p_hat   <- pmin(pmax(1 / (1 + exp(-linpred)), 1e-8), 1 - 1e-8)
ll      <- sum(y * log(p_hat) + (1 - y) * log(1 - p_hat))
# 5) compute BIC
df_beta <- ncol(X)
df_eta  <- 1 + as.integer(eta1 != 0)
df_tot  <- df_beta + df_eta
bic_val <- -2 * ll + df_tot * log(n)
if (!is.finite(bic_val)) bic_val <- 1e10
return(bic_val)
}
# Chunk 9
#––– GRID SEARCH over eta0, eta1 –––
eta0_grid <- seq(-10, -2, by = 0.25)   # baseline penalty
eta1_grid <- seq(-2,  2, by = 0.25)   # penalty adjustment for z_j
# build all combinations
grid <- expand.grid(eta0 = eta0_grid, eta1 = eta1_grid)
# evaluate BIC at each point using z_j
grid$bic <- mapply(
compute_bic,
eta0     = grid$eta0,
eta1     = grid$eta1,
MoreArgs = list(X = X, y = y, z_j = z_j)
)
# find the best hyperparameter combination
best_idx <- which.min(grid$bic)
best     <- grid[best_idx, ]
cat(sprintf(
"→ Best BIC: η₀ = %.3f, η₁ = %.3f, BIC = %.1f\n",
best$eta0, best$eta1, best$bic
))
setwd("/Users/tirdodbehbehani/Desktop/Soccer Analytics/football_market_value_and_player_archetypes/Data/Train, Test")
# Chunk 1: setup
knitr::opts_chunk$set(
tidy = TRUE,      # Clean up code output
tidy.opts = list(width.cutoff = 50), # Wrap code output at 60 characters
comment = NA      # Remove comments from code output
)
rm(list=ls())
# Load libraries
library(tidyverse)    # Data manipulation and visualization
library(factoextra)   # Visualization for clustering
library(broom)        # Tidy model outputs
library(knitr)        # Pretty tables
library(dplyr)
library(glmnet)
library(caret)
library(ggplot2)
library(gridExtra)
# remotes::install_github("emilhvitfeldt/extrasteps")
library(recipes)
library(extrasteps)
library(modeldata)
# Chunk 2
set.seed(123)
# Load the prepared datasets - 90s
X_train_90s <- read_csv("train_test_data/X_train_90s.csv", show_col_types = FALSE)[,-1]
set.seed(123)
# Load the prepared datasets - 90s
X_train_90s <- read_csv("X_train_90s.csv", show_col_types = FALSE)[,-1]
y_train_90s <- as.numeric(unlist(read_csv("y_train_90s.csv", show_col_types = FALSE)[,-1]))
X_test_90s <- read_csv("X_test_90s.csv", show_col_types = FALSE)[,-1]
y_test_90s <- as.numeric(unlist(read_csv("y_test_90s.csv", show_col_types = FALSE)[,-1]))
# Load the prepared datasets - totals
X_train_totals <- read_csv("X_train_totals.csv", show_col_types = FALSE)[,-1]
y_train_totals <- as.numeric(unlist(read_csv("y_train_totals.csv", show_col_types = FALSE)[,-1]))
X_test_totals <- read_csv("X_test_totals.csv", show_col_types = FALSE)[,-1]
y_test_totals <- as.numeric(unlist(read_csv("y_test_totals.csv", show_col_types = FALSE)[,-1]))
# Replace dashes with underscores in column names for consistency
colnames(X_train_90s) <- gsub("-", "_", colnames(X_train_90s))
colnames(X_test_90s) <- gsub("-", "_", colnames(X_test_90s))
colnames(X_train_totals) <- gsub("-", "_", colnames(X_train_totals))
colnames(X_test_totals) <- gsub("-", "_", colnames(X_test_totals))
# Log-transform the target variables
y_train_totals_log <- log(y_train_totals)
y_test_totals_log <- log(y_test_totals)
y_train_90s_log <- log(y_train_90s)
y_test_90s_log <- log(y_test_90s)
# Preprocess (Center & Scale) the training data
preProc_90s <- preProcess(X_train_90s, method = c("center", "scale"))
X_train_90s_scaled <- predict(preProc_90s, X_train_90s)
X_test_90s_scaled <- predict(preProc_90s, X_test_90s)
preProc_totals <- preProcess(X_train_totals, method = c("center", "scale"))
X_train_totals_scaled <- predict(preProc_totals, X_train_totals)
X_test_totals_scaled <- predict(preProc_totals, X_test_totals)
# Try robust scaling
# Define the recipe for robust scaling (90s dataset)
#recipe_90s <- recipe(~ ., data = X_train_90s) |>
#  step_robust(all_predictors(), range = c(0.1, 0.9)) |>
#  prep()
# Apply the recipe to scale the training and test data (90s dataset)
#X_train_90s_scaled <- bake(recipe_90s, new_data = X_train_90s)
#X_test_90s_scaled <- bake(recipe_90s, new_data = X_test_90s)
# Define the recipe for robust scaling (Totals dataset)
#recipe_totals <- recipe(~ ., data = X_train_totals) |>
#  step_robust(all_predictors(), range = c(0.1, 0.9)) |>
#  prep()
# Apply the recipe to scale the training and test data (Totals dataset)
#X_train_totals_scaled <- bake(recipe_totals, new_data = X_train_totals)
#X_test_totals_scaled <- bake(recipe_totals, new_data = X_test_totals)
set.seed(123)
# Plot the original market values (Totals)
p1 <- ggplot(data.frame(y = y_train_totals), aes(x = y)) +
geom_histogram(binwidth = 1e7, fill = "#4C72B0", alpha = 0.8, color = "white", size = 0.3) +
labs(
title = "Original Market Values (Totals)",
x = "Market Value (in millions)",
y = "Frequency"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
axis.title = element_text(size = 16),
axis.text = element_text(size = 12),
panel.grid.major = element_line(color = "grey80", linetype = "dotted"),
panel.grid.minor = element_blank()
)
# plot thelog-transformed market values (Totals)
p2 <- ggplot(data.frame(y = y_train_totals_log), aes(x = y)) +
geom_histogram(binwidth = 0.1, fill = "#55A868", alpha = 0.8, color = "white", size = 0.3) +
labs(
title = "Log-Transformed Market Values (Totals)",
x = "Log(Market Value)",
y = "Frequency"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
axis.title = element_text(size = 16),
axis.text = element_text(size = 12),
panel.grid.major = element_line(color = "grey80", linetype = "dotted"),
panel.grid.minor = element_blank()
)
# plot next to each other
p1
p2
set.seed(123)
#summary stats for original and log-transformed target variables
summary_targets <- data.frame(
Dataset = rep(c("Training", "Test"), each = 2),
Scale = rep(c("Original", "Log"), times = 2),
Mean = c(format(mean(y_train_totals), big.mark = ",", scientific = FALSE),
round(mean(y_train_totals_log), 2),
format(mean(y_test_totals), big.mark = ",", scientific = FALSE),
round(mean(y_test_totals_log), 2)),
Median = c(format(median(y_train_totals), big.mark = ",", scientific = FALSE),
round(median(y_train_totals_log), 2),
format(median(y_test_totals), big.mark = ",", scientific = FALSE),
round(median(y_test_totals_log), 2)),
SD = c(format(sd(y_train_totals), big.mark = ",", scientific = FALSE),
round(sd(y_train_totals_log), 2),
format(sd(y_test_totals), big.mark = ",", scientific = FALSE),
round(sd(y_test_totals_log), 2)),
Min = c(format(min(y_train_totals), big.mark = ",", scientific = FALSE),
round(min(y_train_totals_log), 2),
format(min(y_test_totals), big.mark = ",", scientific = FALSE),
round(min(y_test_totals_log), 2)),
Max = c(format(max(y_train_totals), big.mark = ",", scientific = FALSE),
round(max(y_train_totals_log), 2),
format(max(y_test_totals), big.mark = ",", scientific = FALSE),
round(max(y_test_totals_log), 2))
)
print(summary_targets)
# Extract mean and std from preprocessing
age_mean <- preProc_totals$mean["Age"]
age_sd <- preProc_totals$std["Age"]
# Coefficients from Adaptive Lasso (replace with your actual values)
coef_age <- 1.37
coef_age2 <- -1.88
# Compute peak age
peak_age <- age_mean + (-coef_age / (2 * coef_age2)) * age_sd
print(paste("Estimated peak market value occurs around age:", round(peak_age, 2)))
"Age" %in% colnames(X_train_totals)
names(X_train_totals)
# Extract mean and std from preprocessing
age_mean <- preProc_totals$mean["age"]
age_sd <- preProc_totals$std["age"]
# Coefficients from Adaptive Lasso (replace with your actual values)
coef_age <- 1.37
coef_age2 <- -1.88
# Compute peak age
peak_age <- age_mean + (-coef_age / (2 * coef_age2)) * age_sd
print(paste("Estimated peak market value occurs around age:", round(peak_age, 2)))
